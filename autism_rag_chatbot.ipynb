{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "๐ Notebook Section 1\n",
        "\n",
        "Document Ingestion & Chunking Pipeline (Arabic-Friendly)"
      ],
      "metadata": {
        "id": "Kt2s3CteLyjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section ingests Arabic PDF documents, extracts and normalizes text, then splits it into overlapping semantic chunks suitable for embedding and retrieval in a RAG system."
      ],
      "metadata": {
        "id": "wysGcPHzPLG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVnD6il5LeJS"
      },
      "outputs": [],
      "source": [
        "# Colab-ready robust PDF -> chunks pipeline (Arabic-friendly)\n",
        "# ุงูุชุนูููุงุช: ุงุฑูุน ZIP ุฅูู /content ุซู ุดุบูู ุงูุฎููุฉ.\n",
        "!pip install PyPDF2 pdfplumber --quiet\n",
        "\n",
        "import zipfile, os, re, time\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# ุชุนุฏูู ูุฐุง ุงููุณุงุฑ ุฅุฐุง ุฑูุนุช ุงูููู ูู ููุงู ุขุฎุฑ\n",
        "ZIP_PATH = Path('/content/100-day-kit-arabic_0.zip')\n",
        "EXTRACT_DIR = Path('/content/extracted_pdfs')\n",
        "OUT_CSV = Path('/content/processed_chunks.csv')\n",
        "\n",
        "# ุฅุนุฏุงุฏ\n",
        "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) ูู ุงูุถุบุท\n",
        "print(\"Unzipping:\", ZIP_PATH)\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    zf.extractall(EXTRACT_DIR)\n",
        "print(\"Extracted to:\", EXTRACT_DIR)\n",
        "\n",
        "# 2) ุฌูุน ูู ูููุงุช PDF\n",
        "pdf_paths = sorted(EXTRACT_DIR.rglob('*.pdf'))\n",
        "print(\"Found PDFs:\", len(pdf_paths))\n",
        "\n",
        "# 3) ุฏูุงู ุงุณุชุฎุฑุงุฌ ุณุฑูุนุฉ ูููุซููุฉ\n",
        "def extract_text_pypdf2(path):\n",
        "    parts = []\n",
        "    try:\n",
        "        reader = PdfReader(str(path))\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                txt = page.extract_text()\n",
        "                if txt:\n",
        "                    parts.append(txt)\n",
        "            except Exception:\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        # ุทุจุงุนุฉ ุงูุฎุทุฃ ููู ูุง ุชููู ุงูุชุดุบูู\n",
        "        print(f\"[PyPDF2 error] {path.name}: {e}\")\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def extract_text_pdfplumber(path):\n",
        "    parts = []\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                try:\n",
        "                    txt = page.extract_text()\n",
        "                    if txt:\n",
        "                        parts.append(txt)\n",
        "                except Exception:\n",
        "                    continue\n",
        "    except Exception as e:\n",
        "        print(f\"[pdfplumber error] {path.name}: {e}\")\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def normalize_arabic_text(text):\n",
        "    txt = text.replace('\\r', '\\n')\n",
        "    txt = re.sub(r'\\n\\s+\\n', '\\n', txt) # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ุงูููุฑุฑุฉ\n",
        "    txt = re.sub(r'[ \\t]+', ' ', txt) # ุชูุญูุฏ ุงููุณุงูุงุช (ุจุฏู ูุณุงูุชูู ุชุตุจุญ ูุงุญุฏุฉ)\n",
        "    txt = re.sub(r'\\n{2,}', '\\n\\n', txt)\n",
        "    # ุญุฐู ุฃุฑูุงู ุตูุญุงุช ุดุงุฆุนุฉ\n",
        "    txt = re.sub(r'(?i)page\\s*\\d+|\\d+\\s*/\\s*\\d+|\\bุตูุญุฉ\\b\\s*\\d+', '', txt) # ุฅุฒุงูุฉ ูููุงุช ูุซู \"Page 1\" ุฃู \"ุตูุญุฉ 5\" ูุฃููุง ุชุดูุด ุนูู ุงูุจูุช\n",
        "    lines = [ln.strip() for ln in txt.splitlines()]\n",
        "    lines = [ln for ln in lines if len(ln) > 1]\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "# 4) ุชูุณูู ุฐูู ุฅูู ููุงุทุน (chunking)\n",
        "SENT_DELIMS = r'(?<=[\\.\\?\\!\\ุ\\!ุ\\n])\\s+'\n",
        "TARGET_CHARS = 1000\n",
        "OVERLAP_CHARS = 200\n",
        "\n",
        "# ุณููุดุฆ CSV ุจุดูู ุชุฑุงููู: ููุชุจ ุตููู ูู ููู ููุฑุงู ูุชูุงุฏู ููุฏุงู ุงูุนูู ุฅุฐุง ุญุฏุซ ุฎุทุฃ ูุงุญูุงู\n",
        "rows_written = 0\n",
        "with open(OUT_CSV, 'w', encoding='utf-8', newline='') as fout:\n",
        "    import csv\n",
        "    writer = csv.DictWriter(fout, fieldnames=['id','source_pdf','chunk_index','text','char_len'])\n",
        "    writer.writeheader()\n",
        "    chunk_id = 0\n",
        "\n",
        "    for p in pdf_paths:\n",
        "        t0 = time.time()\n",
        "        print(f\"\\nProcessing {p.name} ...\", end=' ')\n",
        "        # 1) ุญุงูู ุงุณุชุฎุฑุงุฌ ุณุฑูุนุง ุจู PyPDF2\n",
        "        raw = extract_text_pypdf2(p)\n",
        "        # 2) ุฅู ูุงูุช ุงููุชูุฌุฉ ุถุนููุฉุ ุฌุฑุจ pdfplumber\n",
        "        if not raw or len(raw) < 50:\n",
        "            raw = extract_text_pdfplumber(p)\n",
        "        if not raw or len(raw) < 50:\n",
        "            print(\"SKIPPED (no text)\")\n",
        "            continue\n",
        "        norm = normalize_arabic_text(raw)\n",
        "        # ุชูุณูู ุฅูู \"ุฌูู\" ุซู ุชุฌููุน ุฅูู chunks\n",
        "        parts = re.split(SENT_DELIMS, norm)\n",
        "        cur = \"\"\n",
        "        chunk_idx = 0\n",
        "        for part in parts:\n",
        "            if not part or len(part.strip())==0:\n",
        "                continue\n",
        "            part = part.strip()\n",
        "            if len(cur) + len(part) <= TARGET_CHARS or len(cur) == 0:\n",
        "                cur = (cur + \" \" + part).strip() if cur else part\n",
        "            else:\n",
        "                writer.writerow({\n",
        "                    'id': f\"chunk_{chunk_id:06d}\",\n",
        "                    'source_pdf': str(p.name),\n",
        "                    'chunk_index': chunk_idx,\n",
        "                    'text': cur,\n",
        "                    'char_len': len(cur)\n",
        "                })\n",
        "                rows_written += 1\n",
        "                chunk_id += 1\n",
        "                chunk_idx += 1\n",
        "                # overlap\n",
        "                overlap = cur[-OVERLAP_CHARS:] if OVERLAP_CHARS < len(cur) else cur\n",
        "                cur = (overlap + \" \" + part).strip()\n",
        "        if cur and len(cur.strip())>0:\n",
        "            writer.writerow({\n",
        "                'id': f\"chunk_{chunk_id:06d}\",\n",
        "                'source_pdf': str(p.name),\n",
        "                'chunk_index': chunk_idx,\n",
        "                'text': cur,\n",
        "                'char_len': len(cur)\n",
        "            })\n",
        "            rows_written += 1\n",
        "            chunk_id += 1\n",
        "        print(f\"done in {time.time()-t0:.1f}s, chars={len(raw)} (rows_written={rows_written})\")\n",
        "\n",
        "print(f\"\\nAll done. CSV saved to: {OUT_CSV}  (total chunks: {rows_written})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "๐ Notebook Section 2\n",
        "\n",
        "Embedding, Vector Store Construction & RAG Query Engine"
      ],
      "metadata": {
        "id": "vktmBeL5MRbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section converts text chunks into vector embeddings, stores them in a persistent vector database, and builds a Retrieval-Augmented Generation (RAG) pipeline for grounded question answering."
      ],
      "metadata": {
        "id": "wfZ2gXI0OwfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ุชุญุฏูุซ ูุชุซุจูุช ุงูููุชุจุงุช (ุชุดูู ุงูุญู ูุชุนุงุฑุถ ุงูุฅุตุฏุงุฑุงุช)\n",
        "!pip install -U langchain-google-genai langchain-community chromadb sentence-transformers langchain-chroma pandas==2.2.2 --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import chromadb\n",
        "from pathlib import Path\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# --- ุงูุฅุนุฏุงุฏุงุช ุงูุฃุณุงุณูุฉ ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"-put your GEMINI_API_KEY here\"\n",
        "OUT_CSV = Path('/content/processed_chunks.csv')\n",
        "CHROMA_DB_PATH = \"/content/autism_db_fixed\"\n",
        "COLLECTION_NAME = \"autism_collection\"\n",
        "\n",
        "# 2. ุชุญููู ูููุฐุฌ ุงูุชุถููู (E5 ุงููุฎุตุต ููุบุฉ ุงูุนุฑุจูุฉ)\n",
        "print(\"โณ ุฌุงุฑู ุชุญููู ูููุฐุฌ ุงูุชุถููู...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')\n",
        "\n",
        "# 3. ุฏุงูุฉ ุจูุงุก ุฃู ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช\n",
        "def get_vectorstore():\n",
        "    # ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ูุงุนุฏุฉ ุงูุจูุงูุงุช ููุฌูุฏุฉ ูุณุจูุงู\n",
        "    if os.path.exists(CHROMA_DB_PATH) and os.listdir(CHROMA_DB_PATH):\n",
        "        print(\"โ ุชู ุงูุนุซูุฑ ุนูู ูุงุนุฏุฉ ุจูุงูุงุช ุฌุงูุฒุฉุ ูุชู ุชุญููููุง ุงูุขู...\")\n",
        "        return Chroma(\n",
        "            persist_directory=CHROMA_DB_PATH,\n",
        "            embedding_function=embeddings,\n",
        "            collection_name=COLLECTION_NAME\n",
        "        )\n",
        "\n",
        "    # ุฅุฐุง ูู ุชูู ููุฌูุฏุฉุ ูุจุฏุฃ ุจุจูุงุฆูุง ุจุงูุฏูุนุงุช (Batch Processing) ูุชูููุฑ ุงูุฐุงูุฑุฉ\n",
        "    if not OUT_CSV.exists():\n",
        "        raise FileNotFoundError(f\"โ๏ธ ููู {OUT_CSV} ุบูุฑ ููุฌูุฏ! ูุฑุฌู ุชุดุบูู ุฌุฒุก ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ุฃููุงู.\")\n",
        "\n",
        "    print(f\"๐ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุบูุฑ ููุฌูุฏุฉ. ุจุฏุก ุงููุนุงูุฌุฉ ูู ููู CSV: {OUT_CSV}\")\n",
        "    os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
        "\n",
        "    # ุชููุฆุฉ ุนููู ChromaDB\n",
        "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "    vectorstore = Chroma(client=client, collection_name=COLLECTION_NAME, embedding_function=embeddings)\n",
        "\n",
        "    # ูุฑุงุกุฉ ุงูููู ุจุงูุฏูุนุงุช ูุชุฌูุจ ุงูุชูุงุก ุงูุฐุงูุฑุฉ (RAM)\n",
        "    reader = pd.read_csv(OUT_CSV, chunksize=500)\n",
        "    for i, chunk in enumerate(reader):\n",
        "        chunk = chunk.fillna('')\n",
        "        texts = chunk['text'].astype(str).tolist()\n",
        "        metadatas = chunk[['source_pdf']].to_dict('records')\n",
        "        vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
        "        print(f\"โ ุชู ุญูุธ ุงูุฏูุนุฉ {i+1} (ุฅุฌูุงูู ุงูุณุฌูุงุช: {(i+1)*500})\")\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# 4. ุชุดุบูู ุจูุงุก/ุชุญููู ุงููุงุนุฏุฉ\n",
        "try:\n",
        "    vectorstore = get_vectorstore()\n",
        "\n",
        "    # 5. ุฅุนุฏุงุฏ Gemini (ุญู ูุดููุฉ 404 ุนุจุฑ ุชุญุฏูุฏ api_version)\n",
        "    print(\"โณ ุฌุงุฑู ุงูุงุชุตุงู ุจูุญุฑู Gemini...\")\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0.1,\n",
        "        client_options={\"api_version\": \"v1\"} # ูุฐุง ุงูุณุทุฑ ูุญู ูุดููุฉ 404 ููุงุฆูุงู\n",
        "    )\n",
        "\n",
        "    # 6. ุจูุงุก ูุญุฑู ุงูุฅุฌุงุจุฉ (RAG Chain)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    template = \"\"\"ุฃูุช ุฎุจูุฑ ูุชุฎุตุต ูู ุงุถุทุฑุงุจ ุทูู ุงูุชูุญุฏ. ุฃุฌุจ ุจุฏูุฉ ููุฏูุก ุจูุงุกู ุนูู ุงูุณูุงู ุงููุฑูู ููุท.\n",
        "    ุฅุฐุง ูู ุชุฌุฏ ุงูุฅุฌุงุจุฉุ ุฃุฎุจุฑ ุงููุณุชุฎุฏู ุฃู ุงููุนูููุฉ ุบูุฑ ูุชููุฑุฉ ูู ุงููููุงุช ุงููุฑููุฉ ุญุงููุงู.\n",
        "\n",
        "    ุงูุณูุงู:\n",
        "    {context}\n",
        "\n",
        "    ุณุคุงู ุงููุณุชุฎุฏู: {question}\n",
        "\n",
        "    ุงูุฅุฌุงุจุฉ:\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    print(\"\\n๐ ุงููุธุงู ุฌุงูุฒ ุชูุงูุงู!\")\n",
        "\n",
        "    # 7. ูุงุฌูุฉ ุงููุญุงุฏุซุฉ ุงูุจุณูุทุฉ\n",
        "    user_input = input(\"\\nุงุณุฃููู ุฃู ุดูุก ุนู ุงูุชูุญุฏ (ุฃู ุงูุชุจ 'ุฎุฑูุฌ'): \")\n",
        "    if user_input.lower() not in ['ุฎุฑูุฌ', 'exit', 'quit']:\n",
        "        print(\"\\nโณ ุฌุงุฑู ุงูุจุญุซ ุนู ุงูุฅุฌุงุจุฉ...\")\n",
        "        response = rag_chain.invoke(user_input)\n",
        "        print(f\"\\nุงูุฅุฌุงุจุฉ:\\n{response.content}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nโ ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุชุดุบูู: {e}\")"
      ],
      "metadata": {
        "id": "lE1j4kImMR3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "๐ Notebook Section 3\n",
        "\n",
        "Persistent Vector Store Loading & Inference"
      ],
      "metadata": {
        "id": "oLCqBkL1PY3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section loads a previously built and persisted vector database from disk (Google Drive) and performs retrieval-augmented inference without re-computing embeddings, enabling fast and cost-efficient chatbot execution."
      ],
      "metadata": {
        "id": "ehkAJrEjPZxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import drive\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 1. ุฑุจุท Google Drive ูุชุญููู ุงูุจูุงูุงุช\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/autism_rag_metadata\"\n",
        "\n",
        "# 2. ุฅุนุฏุงุฏ Gemini (ุงูููุชุงุญ ุงูุฐู ูุฏูุชู ุณุงุจูุงู)\n",
        "# ุชุฃูุฏ ูู ุนุฏู ูุฌูุฏ ูุณุงูุงุช ูู ุงูููุชุงุญ\n",
        "API_KEY = \"put your API_KEY here\"\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# 3. ุชุญููู ูููุฐุฌ ุงูุชุถููู ููุงุนุฏุฉ ุงูุจูุงูุงุช (ุงูู 1500 ููุทุน)\n",
        "print(\"โณ ุฌุงุฑู ุงุณุชุนุงุฏุฉ ุงูู 1500 ููุทุน ูู Google Drive...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=DRIVE_SAVE_PATH,\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"autism_collection\"\n",
        ")\n",
        "print(\"โ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุฌุงูุฒุฉ.\")\n",
        "\n",
        "# 4. ุฏุงูุฉ ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ (ุชุณุชุฎุฏู ุงูููุฏูู ุงููุณุชูุฑ gemini-1.5-flash)\n",
        "def ask_gemini_direct(query):\n",
        "    try:\n",
        "        # ุงูุจุญุซ ูู ุงูู 1500 ููุทุน\n",
        "        docs = vectorstore.similarity_search(query, k=3)\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "        # ุงุณุชุฎุฏุงู ุงูููุฏูู ูุจุงุดุฑุฉ ูุชุฌูุจ ุฃุฎุทุงุก 404\n",
        "        # ููุงุญุธุฉ: ุฅุฐุง ูุดู 1.5-flashุ ุงุณุชุฎุฏู 'gemini-pro'\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "        full_prompt = f\"\"\"ุฃูุช ุฎุจูุฑ ูู ุงูุชูุญุฏ. ุฃุฌุจ ุจูุงุกู ุนูู ุงููุนูููุงุช ุงููุฑููุฉ ููุท.\n",
        "\n",
        "        ุงููุนูููุงุช ุงููุณุชุฎุฑุฌุฉ ูู ูููุงุชู:\n",
        "        {context}\n",
        "\n",
        "        ุณุคุงู ุงููุณุชุฎุฏู: {query}\n",
        "\n",
        "        ุงูุฅุฌุงุจุฉ:\"\"\"\n",
        "\n",
        "        response = model.generate_content(full_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        # ุฅุฐุง ุญุฏุซ ุฎุทุฃ 404ุ ูุญุงูู ุงุณุชุฎุฏุงู ุงููุณุฎุฉ ุงูุฃูุฏู ูุงูุฃูุซุฑ ุชูุงููุงู\n",
        "        if \"404\" in str(e):\n",
        "            model_backup = genai.GenerativeModel('gemini-pro')\n",
        "            response = model_backup.generate_content(full_prompt)\n",
        "            return response.text\n",
        "        return f\"โ ุญุฏุซ ุฎุทุฃ: {e}\"\n",
        "\n",
        "# 5. ุชุดุบูู ุงููุญุงุฏุซุฉ\n",
        "print(\"-\" * 30)\n",
        "user_input = input(\"ุงุณุฃููู ุนู ุงูุชูุญุฏ ูู ูุงูุน ูููุงุชู: \")\n",
        "if user_input:\n",
        "    print(\"โณ ุฌุงุฑู ุชุญููู ุงููุนูููุงุช ูุชูููุฏ ุงูุฅุฌุงุจุฉ...\")\n",
        "    result = ask_gemini_direct(user_input)\n",
        "    print(f\"\\nุงูุฅุฌุงุจุฉ:\\n{result}\")"
      ],
      "metadata": {
        "id": "WV2KugPEPb36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "๐ Notebook Section 4\n",
        "\n",
        "RAG Chatbot Runtime with GPT-4o Mini"
      ],
      "metadata": {
        "id": "-8_HEAprRLM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section performs real-time retrieval-augmented generation using a persisted vector database and GPT-4o-mini.\n",
        "It retrieves the most relevant chunks from your database, assembles them as context, and generates precise answers grounded in your documents.\n",
        "\n",
        "Uses LangChain abstractions (retriever | prompt | llm)\n",
        "\n",
        "Shows snippets for transparency\n",
        "\n",
        "Guards against hallucination by instructing the model to answer based only on the context"
      ],
      "metadata": {
        "id": "-k8TLeUdRK6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ุชุซุจูุช ุงูููุชุจุงุช (ููุชุฃูุฏ ูู ุชูุงูู ุงูุฅุตุฏุงุฑุงุช)\n",
        "print(\"โณ ุฌุงุฑู ุงูุชุฃูุฏ ูู ุงูููุชุจุงุช...\")\n",
        "!pip install -U langchain-openai langchain-chroma langchain-community chromadb sentence-transformers --quiet\n",
        "\n",
        "import os\n",
        "from google.colab import drive, userdata\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 2. ุฑุจุท Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 3. ุฅุนุฏุงุฏ ุงูููุงุชูุญ ูุงููุณุงุฑุงุช\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except:\n",
        "    # ุฅุฐุง ูู ุชูู ุชุณุชุฎุฏู Secretsุ ุถุน ููุชุงุญู ููุง\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"put your OPENAI_API_KEY here\"\n",
        "\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/autism_rag_metadata\"\n",
        "\n",
        "# 4. ุชุญููู ุงูุชุถูููุงุช ููุงุนุฏุฉ ุงูุจูุงูุงุช\n",
        "print(\"๐ ุฌุงุฑู ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=DRIVE_SAVE_PATH,\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"autism_collection\"\n",
        ")\n",
        "\n",
        "# 5. ุฅุนุฏุงุฏ ุงูููุฏูู\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "\n",
        "# 6. ุจูุงุก ูุญุฑู ุงูุจุญุซ (Retriever)\n",
        "# ุงุณุชุฎุฏููุง k=5 ูุฒูุงุฏุฉ ูููุฉ ุงููุนูููุงุช ุงููุณุชุฎุฑุฌุฉ\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "template = \"\"\"ุฃูุช ุฎุจูุฑ ูุชุฎุตุต ูู ุงุถุทุฑุงุจ ุทูู ุงูุชูุญุฏ. ุฃุฌุจ ุจูุงุกู ุนูู ุงูุณูุงู ุงููุฑูู ููุท.\n",
        "\n",
        "ุงูุณูุงู:\n",
        "{context}\n",
        "\n",
        "ุงูุณุคุงู: {question}\n",
        "\n",
        "ุงูุฅุฌุงุจุฉ:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        ")\n",
        "\n",
        "# 7. ุงูุฏุงูุฉ ุงููุตุญุญุฉ (ุชู ุชุบููุฑ get_relevant_documents ุฅูู invoke)\n",
        "def ask_and_verify(question):\n",
        "    print(f\"\\n๐ ุงูุณุคุงู: {question}\")\n",
        "\n",
        "    # ุงูุชุนุฏูู ููุง: ุงุณุชุฎุฏุงู invoke ุจุฏูุงู ูู get_relevant_documents\n",
        "    try:\n",
        "        relevant_docs = retriever.invoke(question)\n",
        "\n",
        "        print(\"\\n--------------------------------------------------\")\n",
        "        print(f\"๐ ุชู ุงูุนุซูุฑ ุนูู {len(relevant_docs)} ููุงุทุน ุฐุงุช ุตูุฉ ูู ูููุงุชู:\")\n",
        "        for i, doc in enumerate(relevant_docs):\n",
        "            snippet = doc.page_content[:150].replace('\\n', ' ')\n",
        "            print(f\"[{i+1}] {snippet}...\")\n",
        "        print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "        # ุชูููุฏ ุงูุฅุฌุงุจุฉ\n",
        "        print(\"๐ค ุฅุฌุงุจุฉ GPT-4o-mini:\")\n",
        "        response = rag_chain.invoke(question)\n",
        "        print(response.content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"โ ุญุฏุซ ุฎุทุฃ: {e}\")\n",
        "\n",
        "# --- ุชุดุบูู ุงูุณุคุงู ---\n",
        "ask_and_verify(\"ูุง ูู ุงููุตุงุฆุญ ุงูุนูููุฉ ููุชุนุงูู ูุน ููุจุงุช ุงูุบุถุจุ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlMX2sMbRLvF",
        "outputId": "65e293aa-666b-41a2-e3d6-5366d57710a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "โณ ุฌุงุฑู ุงูุชุฃูุฏ ูู ุงูููุชุจุงุช...\n",
            "๐ ุฌุงุฑู ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช...\n",
            "\n",
            "๐ ุงูุณุคุงู: ูุง ูู ุงููุตุงุฆุญ ุงูุนูููุฉ ููุชุนุงูู ูุน ููุจุงุช ุงูุบุถุจุ\n",
            "\n",
            "--------------------------------------------------\n",
            "๐ ุชู ุงูุนุซูุฑ ุนูู 5 ููุงุทุน ุฐุงุช ุตูุฉ ูู ูููุงุชู:\n",
            "[1] ุนู ุงุฃููุงุฏููู ูุงูุจูุฆู ูุงุงูุฌุชูุงุนู ูุงูุญุณู ุงูููุงุณุจ ูุงูุชุนุฏูู ุนูู ุงูุจูุฆุฉ ูุงูุชููุนุงุช ูู ุงูุทุฑููุฉ ุงุฃููุซุฑ ูุนุงููุฉ. ููุฌุฏ ุงูุนุฏูุฏ ูู ุงุงูุณุชุฑุงุชูุฌูุงุช ุงูุชู ูููู ุงุณุชุฎุฏุงูู...\n",
            "[2] ููุณุน: 5 ูุตุงุฆุญ ูููุงูุฏูู 1. ุชุนูู ุฃู ุชููู ุฃูุถูููุงุตุฑ ููููู ุฃู ุชููู ุนููู ูุทููู. ูู ูุงุณุน ุงุงูุทุงูุน. ุงุณุชูุฏ ูู ุฌููุน ุงูุฎุฏูุงุช ุงููุชุงุญุฉ ูู ูู ูุฌุชูุนู. ุณูู ุชูุงุจู ุงููู...\n",
            "[3] ุฒุฏุงุฏ ููุจุงุช ุงูุบุถุจ ูุฏู ุงุฃูุทูุงู ุงูุฐูู ูุนุงููู ูู ุขุงูู .ุงููุดููุฉ ุงูุณููููุฉ ูุนุงูุฌูุง ุบู ุงุฃูุณูุงู ุฃู ุงููุนุฏุฉ. ููุง ูุฏ ุชูุงูุญุธ ุงูุนุตุจูุฉ ูุฏู ุงุฃูุทูุงู ุงูุฐูู ูุนุงููู ูู ุญุณ...\n",
            "[4] ุงุฏุฆุฉ ุชูุงูู ูุฌุจุฉ ุฎูููุฉ ุงุฑุชุฏุงุก ุณุชุฑุฉ ูุซููุฉ ูุถุบ ุงูุนููุฉ ุฃู ุชูุงูู ุงููุดุฑูุจุงุช ุจุงููุตุงุตุงุช ุงููุตููุนุฉ ูู ุงูููููู ุฃู ุงุฃูุทุนูุฉ ุงููุชูุชุชุฉ ุฃู ุงููุทุงุทูุฉ ุฅุจูุงุก ุงุฅูุถุงุกุฉ ุฎุงูุช...\n",
            "[5] ูู ุงูุญุตูู ุนูู ูุณุท ูู ุงูุฑุงุญุฉ. ุฅุฐุง ููุช ุชูุงู ุจุดูู ููุชุธูุ ูุณุชููู ูุณุชุนุฏู ุง ุจุดูู ุฃูุถู ุงูุชุฎุงุฐ ูุฑุงุฑุงุช ุฌูุฏุฉุ ููู ุตุจูุฑู ุง ุฃูุซุฑ ูุน ุทููู ูุฃูุซุฑ ูุฏุฑุฉ ุนูู ุงูุชุนุงูู ูุน...\n",
            "--------------------------------------------------\n",
            "\n",
            "๐ค ุฅุฌุงุจุฉ GPT-4o-mini:\n",
            "ููุชุนุงูู ูุน ููุจุงุช ุงูุบุถุจ ูุฏู ุงูุฃุทูุงู ุฐูู ุงุถุทุฑุงุจ ุทูู ุงูุชูุญุฏุ ูููู ุงุชุจุงุน ุงููุตุงุฆุญ ุงูุนูููุฉ ุงูุชุงููุฉ:\n",
            "\n",
            "1. **ุชูููู ุงูุณููู ุงููุธููู**: ุงูุงุณุชุนุงูุฉ ุจุฃุญุฏ ุงููุชุฎุตุตูู ุงููุฏุฑุจูู ูู ุงูุชุญููู ุงูุณูููู ุงูุชุทุจููู ูุชุญุฏูุฏ ุฃุณุจุงุจ ููุจุงุช ุงูุบุถุจ ูุชุนุฏูู ุงูุจูุฆุฉ ูุชูููู ุงูุชูุชุฑ.\n",
            "\n",
            "2. **ุชุนุฏูู ุงูุจูุฆุฉ**: ุชุบููุฑ ุงูุจูุฆุฉ ุงููุญูุทุฉ ุจุงูุทูู ูุชููู ุฃูุซุฑ ููุงุกูุฉุ ูุซู ุชูููู ุงูุถูุถุงุก ุฃู ุงูุฅุถุงุกุฉ ุงูุณุงุทุนุฉุ ููุง ูุฏ ูุณุงุนุฏ ูู ุชูููู ุงููุญูุฒุงุช ุงูุชู ุชุคุฏู ุฅูู ููุจุงุช ุงูุบุถุจ.\n",
            "\n",
            "3. **ุชุนููู ุงูุชุนุจูุฑ ุนู ุงููุดุงุนุฑ**: ูุณุงุนุฏุฉ ุงูุทูู ุนูู ุชุนูู ููููุฉ ุงูุชุนุจูุฑ ุนู ุฑุบุจุงุชู ููุดุงุนุฑู ุจุทุฑู ููุงุณุจุฉุ ููุง ูุฏ ูููู ูู ุงูุฅุญุจุงุท ุงูุฐู ูุฏ ูุคุฏู ุฅูู ููุจุงุช ุงูุบุถุจ.\n",
            "\n",
            "4. **ูุฑุงูุจุฉ ุงูุตุญุฉ ุงูุฌุณุฏูุฉ**: ุงูุชุฃูุฏ ูู ุฃู ุงูุทูู ูุง ูุนุงูู ูู ูุดุงูู ุตุญูุฉ ูุซู ุขูุงู ุงูุฃุณูุงู ุฃู ุงููุนุฏุฉุ ุญูุซ ูููู ุฃู ุชุคุฏู ูุฐู ุงููุดููุงุช ุฅูู ุฒูุงุฏุฉ ููุจุงุช ุงูุบุถุจ.\n",
            "\n",
            "5. **ุชูููุฑ ุงุณุชุฑุงุชูุฌูุงุช ุชูุฏุฆุฉ**: ุงุณุชุฎุฏุงู ุชูููุงุช ูุซู ุงุฑุชุฏุงุก ุณุชุฑุฉ ูุซููุฉุ ุฃู ุชูุงูู ูุฌุจุงุช ุฎูููุฉุ ุฃู ุงุณุชุฎุฏุงู ุฃุถูุงุก ููููุฉุ ุฃู ุงูุงุณุชูุงุน ุฅูู ุตูุช ุงูุถูุถุงุก ุงูุจูุถุงุก ูููุณุงุนุฏุฉ ูู ุชูุฏุฆุฉ ุงูุทูู.\n",
            "\n",
            "6. **ุชูุฏูุฑ ุงูุงูุชุตุงุฑุงุช ุงูุตุบูุฑุฉ**: ุงูุงุญุชูุงุก ุจุงูุฅูุฌุงุฒุงุช ุงูุตุบูุฑุฉ ุงูุชู ูุญูููุง ุงูุทููุ ููุง ูุนุฒุฒ ุซูุชู ุจููุณู ููููู ูู ููุจุงุช ุงูุบุถุจ.\n",
            "\n",
            "7. **ุงูุงูุชูุงู ุจุงูููู ุงูุฌูุฏ**: ุงูุชุฃูุฏ ูู ุฃู ุงูุทูู ูุญุตู ุนูู ูุณุท ูุงูู ูู ุงููููุ ุญูุซ ุฃู ููุฉ ุงูููู ูุฏ ุชุคุฏู ุฅูู ุฒูุงุฏุฉ ุงูุนุตุจูุฉ ูุงูุณููููุงุช ุงูุณูุจูุฉ.\n",
            "\n",
            "8. **ุงูุชูุงุตู ูุน ุงูุฃูู**: ุงูุชุญุฏุซ ุนู ุงููุดุงุนุฑ ูุงูุชุฌุงุฑุจ ูุน ุงูุดุฑูู ุฃู ุงูุฃุตุฏูุงุก ููุญุตูู ุนูู ุงูุฏุนูุ ูุชุฌูุจ ุชูุฌูู ุงูุบุถุจ ูุญู ุฃูุฑุงุฏ ุงูุฃุณุฑุฉ.\n",
            "\n",
            "9. **ุชุฏููู ุงูููุงุญุธุงุช**: ุงูุงุญุชูุงุธ ุจุฏูุชุฑ ููููุงุช ูุชุชุจุน ุชูุฏู ุงูุทูู ููุง ููุงุณุจู ููุง ูุง ููุงุณุจูุ ููุง ูุณุงุนุฏ ูู ููู ุฃููุงุท ุงูุณููู ุจุดูู ุฃูุถู.\n",
            "\n",
            "10. **ุงุณุชุฎุฏุงู ุงูุฅูุชุฑูุช ุจุญุฐุฑ**: ุงูุงุณุชูุงุฏุฉ ูู ุงููุตุงุฏุฑ ุงูููุซููุฉ ุนูู ุงูุฅูุชุฑูุช ููุญุตูู ุนูู ูุนูููุงุช ุฏูููุฉ ุญูู ุงุถุทุฑุงุจ ุทูู ุงูุชูุญุฏ ูููููุฉ ุงูุชุนุงูู ูุนู. \n",
            "\n",
            "ุชุฐูุฑ ุฃู ูู ุทูู ูุฑูุฏุ ูุฐุง ูู ุงูููู ุชุฌุฑุจุฉ ุงุณุชุฑุงุชูุฌูุงุช ูุฎุชููุฉ ููุนุฑูุฉ ูุง ููุงุณุจ ุทููู ุจุดูู ุฃูุถู.\n"
          ]
        }
      ]
    }
  ]
}